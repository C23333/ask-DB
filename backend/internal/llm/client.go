package llm

import (
	"bufio"
	"bytes"
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"net/http"
	"strings"
	"time"

	"go.uber.org/zap"

	"github.com/yourusername/db_asst/config"
	"github.com/yourusername/db_asst/internal/models"
)

type LLMClient struct {
	provider   string
	apiKey     string
	model      string
	baseURL    string
	timeout    time.Duration
	httpClient *http.Client
	logger     *zap.Logger
}

// NewLLMClient creates a new LLM client with flexible API support
func NewLLMClient(cfg *config.Config, logger *zap.Logger) *LLMClient {
	client := &LLMClient{
		provider: cfg.LLMProvider,
		apiKey:   cfg.LLMAPIKey,
		model:    cfg.LLMModel,
		baseURL:  cfg.LLMBaseURL,
		timeout:  time.Duration(cfg.LLMTimeout) * time.Second,
		httpClient: &http.Client{
			Timeout: time.Duration(cfg.LLMTimeout) * time.Second,
		},
		logger: logger,
	}

	// Set default base URLs if not provided
	if client.baseURL == "" {
		switch client.provider {
		case "openai":
			client.baseURL = "https://api.openai.com/v1"
		case "claude":
			client.baseURL = "https://api.anthropic.com/v1"
		default:
			client.baseURL = cfg.LLMBaseURL
		}
	}

	return client
}

// GenerateSQL generates SQL from natural language
func (c *LLMClient) GenerateSQL(ctx context.Context, req *models.SQLGenerateRequest, schemaContext string, memoryContext string) (*models.SQLGenerateResponse, error) {
	// Build the prompt
	prompt := c.buildSQLGenerationPrompt(req, schemaContext, memoryContext)

	// Call LLM API
	response, err := c.callLLMAPI(ctx, prompt)
	if err != nil {
		c.logger.Error("Failed to call LLM API", zap.Error(err))
		return nil, err
	}

	// Parse the response
	sqlResp := c.parseSQLGenerationResponse(response)

	return sqlResp, nil
}

// GenerateSQLStream streams SQL chunks back via callback.
func (c *LLMClient) GenerateSQLStream(
	ctx context.Context,
	req *models.SQLGenerateRequest,
	schemaContext string,
	memoryContext string,
	onChunk func(string),
) (*models.SQLGenerateResponse, error) {
	if !c.supportsStreaming() {
		resp, err := c.GenerateSQL(ctx, req, schemaContext, memoryContext)
		if err == nil && onChunk != nil {
			onChunk(resp.SQL)
		}
		return resp, err
	}

	prompt := c.buildSQLGenerationPrompt(req, schemaContext, memoryContext)
	var builder strings.Builder

	if err := c.callLLMStream(ctx, prompt, func(chunk string) {
		builder.WriteString(chunk)
		if onChunk != nil {
			onChunk(chunk)
		}
	}); err != nil {
		return nil, err
	}

	sql := strings.TrimSpace(builder.String())
	if sql == "" {
		return nil, fmt.Errorf("no SQL returned from stream")
	}

	return &models.SQLGenerateResponse{
		SQL:       sql,
		Reasoning: "Generated by LLM",
		Source:    "llm",
	}, nil
}

// GenerateGuidance asks LLM to help users refine their query when SQL无法生成.
func (c *LLMClient) GenerateGuidance(ctx context.Context, originalQuery, schemaContext, issue string) (string, error) {
	prompt := c.buildGuidancePrompt(originalQuery, schemaContext, issue)
	response, err := c.callLLMAPI(ctx, prompt)
	if err != nil {
		return "", err
	}
	return strings.TrimSpace(response), nil
}

// DebugSQL generates debugging suggestions for a failed SQL query
func (c *LLMClient) DebugSQL(ctx context.Context, req *models.SQLDebugRequest, schemaContext string) (*models.SQLDebugResponse, error) {
	// Build the debug prompt
	prompt := c.buildDebugPrompt(req, schemaContext)

	// Call LLM API
	response, err := c.callLLMAPI(ctx, prompt)
	if err != nil {
		c.logger.Error("Failed to call LLM API for debugging", zap.Error(err))
		return nil, err
	}

	// Parse the response
	debugResp := c.parseDebugResponse(response)

	return debugResp, nil
}

// callLLMAPI calls the actual LLM API
func (c *LLMClient) callLLMAPI(ctx context.Context, prompt string) (string, error) {
	switch c.provider {
	case "openai":
		return c.callOpenAIAPI(ctx, prompt)
	case "deepseek":
		return c.callOpenAIAPI(ctx, prompt)
	case "claude":
		return c.callClaudeAPI(ctx, prompt)
	default:
		// Generic API call for custom/proxy services
		return c.callGenericAPI(ctx, prompt)
	}
}

func (c *LLMClient) buildGuidancePrompt(originalQuery, schemaContext, issue string) string {
	builder := strings.Builder{}
	builder.WriteString("你是一名经验丰富的 BI 助手，需要帮助用户改进提问方式以便生成正确 SQL。\n")
	builder.WriteString("## 用户当前提问\n")
	builder.WriteString(originalQuery)
	builder.WriteString("\n\n")
	if strings.TrimSpace(schemaContext) != "" {
		builder.WriteString("## 已知的数据库信息\n")
		builder.WriteString(schemaContext)
		builder.WriteString("\n\n")
	}
	if strings.TrimSpace(issue) != "" {
		builder.WriteString("## LLM 或数据库反馈的限制/错误\n")
		builder.WriteString(issue)
		builder.WriteString("\n\n")
	}
	builder.WriteString("请用中文向用户说明原因，并提供 2~3 条可执行的改进建议，例如提供需要的表字段、限定时间范围或换个表述方式。不要返回 SQL，只给出建议。")
	return builder.String()
}

// callOpenAIAPI calls OpenAI API
func (c *LLMClient) callOpenAIAPI(ctx context.Context, prompt string) (string, error) {
	url := fmt.Sprintf("%s/chat/completions", c.baseURL)

	payload := map[string]interface{}{
		"model":       c.model,
		"messages":    []map[string]string{{"role": "user", "content": prompt}},
		"temperature": 0.7,
		"max_tokens":  2000,
	}

	body, err := json.Marshal(payload)
	if err != nil {
		return "", err
	}

	req, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewBuffer(body))
	if err != nil {
		return "", err
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.apiKey))

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		respBody, _ := io.ReadAll(resp.Body)
		return "", fmt.Errorf("OpenAI API error: %d - %s", resp.StatusCode, string(respBody))
	}

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", err
	}

	var result struct {
		Choices []struct {
			Message struct {
				Content string `json:"content"`
			} `json:"message"`
		} `json:"choices"`
	}

	if err := json.Unmarshal(respBody, &result); err != nil {
		return "", err
	}

	if len(result.Choices) > 0 {
		return result.Choices[0].Message.Content, nil
	}

	return "", fmt.Errorf("no response from OpenAI API")
}

// callClaudeAPI calls Claude API
func (c *LLMClient) callClaudeAPI(ctx context.Context, prompt string) (string, error) {
	url := fmt.Sprintf("%s/messages", c.baseURL)

	payload := map[string]interface{}{
		"model":      c.model,
		"max_tokens": 2000,
		"messages": []map[string]string{
			{"role": "user", "content": prompt},
		},
	}

	body, err := json.Marshal(payload)
	if err != nil {
		return "", err
	}

	req, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewBuffer(body))
	if err != nil {
		return "", err
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("x-api-key", c.apiKey)
	req.Header.Set("anthropic-version", "2023-06-01")

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		respBody, _ := io.ReadAll(resp.Body)
		return "", fmt.Errorf("Claude API error: %d - %s", resp.StatusCode, string(respBody))
	}

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", err
	}

	var result struct {
		Content []struct {
			Text string `json:"text"`
		} `json:"content"`
	}

	if err := json.Unmarshal(respBody, &result); err != nil {
		return "", err
	}

	if len(result.Content) > 0 {
		return result.Content[0].Text, nil
	}

	return "", fmt.Errorf("no response from Claude API")
}

// callGenericAPI calls a generic LLM API (for custom/proxy services)
func (c *LLMClient) callGenericAPI(ctx context.Context, prompt string) (string, error) {
	// Assume the service follows OpenAI-compatible API format
	url := fmt.Sprintf("%s/chat/completions", c.baseURL)

	payload := map[string]interface{}{
		"model":       c.model,
		"messages":    []map[string]string{{"role": "user", "content": prompt}},
		"temperature": 0.7,
		"max_tokens":  2000,
	}

	body, err := json.Marshal(payload)
	if err != nil {
		return "", err
	}

	req, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewBuffer(body))
	if err != nil {
		return "", err
	}

	req.Header.Set("Content-Type", "application/json")
	if c.apiKey != "" {
		req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.apiKey))
	}

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		respBody, _ := io.ReadAll(resp.Body)
		return "", fmt.Errorf("LLM API error: %d - %s", resp.StatusCode, string(respBody))
	}

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", err
	}

	var result struct {
		Choices []struct {
			Message struct {
				Content string `json:"content"`
			} `json:"message"`
		} `json:"choices"`
	}

	if err := json.Unmarshal(respBody, &result); err != nil {
		return "", err
	}

	if len(result.Choices) > 0 {
		return result.Choices[0].Message.Content, nil
	}

	return "", fmt.Errorf("no response from LLM API")
}

// buildSQLGenerationPrompt builds a prompt for SQL generation
func (c *LLMClient) buildSQLGenerationPrompt(req *models.SQLGenerateRequest, schemaContext string, memoryContext string) string {
	prompt := fmt.Sprintf(`You are an expert SQL developer. Use the recent conversation memory to continue the thread (it may include errors from earlier attempts). Based on the following database schema and user request, generate a valid Oracle SQL query.

DATABASE SCHEMA:
%s

%s
%s

USER REQUEST:
%s

Please respond with ONLY the SQL query, without any explanation or markdown formatting. The query should:
1. Be valid Oracle SQL syntax
2. Only use SELECT statements
3. Only reference tables and columns that exist in the schema
4. Be optimized for performance

When the latest user query is vague, infer intent from the conversation memory. If the schema genuinely lacks required tables, return a concise ERROR explaining the missing data source (in Chinese). If the user's request can be reformulated using context, propose the best-guess SQL instead of failing.
If you cannot generate a valid query, respond with "ERROR: [reason]"`,
		schemaContext,
		formatMemoryContext(memoryContext),
		formatAdditionalContext(req.Context),
		req.Query)

	return prompt
}

// buildDebugPrompt builds a prompt for SQL debugging
func (c *LLMClient) buildDebugPrompt(req *models.SQLDebugRequest, schemaContext string) string {
	prompt := fmt.Sprintf(`You are an expert Oracle SQL developer. A user tried to execute the following SQL query but got an error. Please analyze the error and suggest a fixed version.

DATABASE SCHEMA:
%s

ORIGINAL QUERY:
%s

ERROR MESSAGE:
%s

Please respond in JSON format with the following structure:
{
  "analysis": "Brief explanation of what went wrong",
  "suggested_sql": "The corrected SQL query (or empty if query is fundamentally wrong)",
  "explanation": "Detailed explanation of the fix"
}

Ensure the suggested SQL:
1. Is valid Oracle SQL syntax
2. Only uses SELECT statements
3. Only references tables and columns that exist in the schema`,
		schemaContext,
		req.SQL,
		req.Error)

	return prompt
}

// parseSQLGenerationResponse parses LLM response for SQL generation
func (c *LLMClient) parseSQLGenerationResponse(response string) *models.SQLGenerateResponse {
	response = c.cleanResponse(response)

	return &models.SQLGenerateResponse{
		SQL:       response,
		Reasoning: "Generated by LLM",
		Source:    "llm",
	}
}

// parseDebugResponse parses LLM response for debugging
func (c *LLMClient) parseDebugResponse(response string) *models.SQLDebugResponse {
	var debugResp struct {
		Analysis     string `json:"analysis"`
		SuggestedSQL string `json:"suggested_sql"`
		Explanation  string `json:"explanation"`
	}

	// Try to parse as JSON
	if err := json.Unmarshal([]byte(response), &debugResp); err != nil {
		// If not JSON, treat the whole response as analysis
		return &models.SQLDebugResponse{
			AnalysisText: response,
			SuggestedSQL: "",
			Explanation:  "",
		}
	}

	return &models.SQLDebugResponse{
		AnalysisText: debugResp.Analysis,
		SuggestedSQL: debugResp.SuggestedSQL,
		Explanation:  debugResp.Explanation,
	}
}

// cleanResponse removes markdown and extra formatting
func (c *LLMClient) cleanResponse(response string) string {
	// Remove markdown code blocks
	response = strings.ReplaceAll(response, "```sql\n", "")
	response = strings.ReplaceAll(response, "```\n", "")
	response = strings.ReplaceAll(response, "```", "")

	return strings.TrimSpace(response)
}

// formatAdditionalContext formats additional context for the prompt
func formatAdditionalContext(context string) string {
	if context == "" {
		return ""
	}
	return fmt.Sprintf("ADDITIONAL CONTEXT:\n%s\n", context)
}

func formatMemoryContext(memoryContext string) string {
	if strings.TrimSpace(memoryContext) == "" {
		return ""
	}
	return fmt.Sprintf("CONVERSATION MEMORY (recent history, auto-compressed; use it to continue the thread even if the latest user query is brief):\n%s\n", memoryContext)
}

func (c *LLMClient) supportsStreaming() bool {
	switch strings.ToLower(c.provider) {
	case "claude":
		return false
	default:
		return true
	}
}

func (c *LLMClient) callLLMStream(ctx context.Context, prompt string, onChunk func(string)) error {
	switch strings.ToLower(c.provider) {
	case "openai", "custom", "deepseek":
		return c.callOpenAIStream(ctx, prompt, onChunk)
	default:
		return fmt.Errorf("streaming not supported for provider %s", c.provider)
	}
}

func (c *LLMClient) callOpenAIStream(ctx context.Context, prompt string, onChunk func(string)) error {
	url := fmt.Sprintf("%s/chat/completions", c.baseURL)

	payload := map[string]interface{}{
		"model":       c.model,
		"messages":    []map[string]string{{"role": "user", "content": prompt}},
		"temperature": 0.7,
		"stream":      true,
	}
	body, err := json.Marshal(payload)
	if err != nil {
		return err
	}

	req, err := http.NewRequestWithContext(ctx, http.MethodPost, url, bytes.NewBuffer(body))
	if err != nil {
		return err
	}
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.apiKey))

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		respBody, _ := io.ReadAll(resp.Body)
		return fmt.Errorf("LLM stream error: %d - %s", resp.StatusCode, string(respBody))
	}

	scanner := bufio.NewScanner(resp.Body)
	buf := make([]byte, 0, 64*1024)
	scanner.Buffer(buf, 1024*1024)

	for scanner.Scan() {
		line := strings.TrimSpace(scanner.Text())
		if line == "" || !strings.HasPrefix(line, "data:") {
			continue
		}

		data := strings.TrimSpace(strings.TrimPrefix(line, "data:"))
		if data == "[DONE]" {
			break
		}

		var chunk struct {
			Choices []struct {
				Delta struct {
					Content string `json:"content"`
				} `json:"delta"`
				Message struct {
					Content string `json:"content"`
				} `json:"message"`
			} `json:"choices"`
		}

		if err := json.Unmarshal([]byte(data), &chunk); err != nil {
			continue
		}

		if len(chunk.Choices) == 0 {
			continue
		}

		content := chunk.Choices[0].Delta.Content
		if content == "" {
			content = chunk.Choices[0].Message.Content
		}
		if content != "" && onChunk != nil {
			onChunk(content)
		}
	}

	if err := scanner.Err(); err != nil && !errors.Is(err, io.EOF) {
		return err
	}

	return nil
}
